{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Guidelines of Sample Code (Pytorch)\n",
    "\n",
    "    See the annotations at every markdown blocks correspoding to each code blocks, and also # TODO annotations. :D\n",
    "\n",
    "# Usage guideline of Jupyter Notebook (If needed)\n",
    "\n",
    "    Installation   : https://jupyter.org/install  \n",
    "    User Document  : https://jupyter-notebook.readthedocs.io/en/latest/user-documentation.html\n",
    "\n",
    "# Test Environment (Recommended)\n",
    "\n",
    "    In test time, we will evaluate the given codes from you with the following version of libraries.  \n",
    "    So, it is highly recommended to use those packages with specific version below.\n",
    "\n",
    "    test environment : pytorch\n",
    "\n",
    "### Packages\n",
    "    python   : 3.8.17  \n",
    "    torch    : 2.0.1   \n",
    "    skimage  : 0.21.0  \n",
    "    cv2      : 4.8.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries (Do not change!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import PIL\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset (Do not change!)\n",
    "\n",
    "### Notice 1\n",
    "    This function do split your dataset of 1000 classes into 10 groups of 100 each.    \n",
    "    So, it is needed to be implemented just once at first to split your dataset for continual learning.   \n",
    "    *Again, you dont need to use this function in every tranining time if you already split your dataset into 10 groups.\n",
    "\n",
    "    Notice the annotation codes below. (You can see this codes in 'main' block.)\n",
    "\n",
    "```python\n",
    "        parser = argparse.ArgumentParser()   \n",
    "        # Change this as 'False' after dividing your datsaet into 10 groups.\n",
    "        parser.add_argument('--div_data',   default = True)  \n",
    "        args = parser.parse_args(args=[])  \n",
    "```\n",
    "\n",
    "### Notice 2\n",
    "    We reshapes all the input data size into constant 128x128.   \n",
    "    Until further notification, use this constant size. \n",
    "\n",
    "```python\n",
    "        # Split input data.  \n",
    "        for i in range(start, end):\n",
    "            for img_idx in range(0, 130):\n",
    "                path = os.path.join(dir, str(i))\n",
    "                path = path + '/' +str(img_idx)+'.png'\n",
    "                img = io.imread(path)\n",
    "                img = cv2.resize(img, (128, 128))  # resize image into 128 x 128 \n",
    "                x_train.append(img)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(validation_num):\n",
    "    # TODO : set dataset path\n",
    "    # TODO : We recommends you to place your code and tranining dataset in the same location.\n",
    "    \n",
    "    dir = './Koh_Young_AI_data/'\n",
    "    \n",
    "\n",
    "    for div_idx in range(0, 10): # Div into 10 groups\n",
    "        # Divide data 0-129 for training, 130-150 for validation.\n",
    "        x_train = []\n",
    "        x_valid = []\n",
    "        y_train = []\n",
    "        y_valid = []\n",
    "        start   = 100*div_idx + 1\n",
    "        end     = 100*div_idx + 100\n",
    "\n",
    "        # Split input data.  \n",
    "        for i in range(start, end):\n",
    "            for img_idx in range(0, 150-validation_num):\n",
    "                path = os.path.join(dir, str(i))\n",
    "                path = path + '/' +str(img_idx)+'.png'\n",
    "                img = io.imread(path)\n",
    "                img = cv2.resize(img, (128, 128))\n",
    "                x_train.append(img)\n",
    "\n",
    "            for img_idx in range(150-validation_num, 150):\n",
    "                path = os.path.join(dir, str(i))\n",
    "                path = path + '/' +str(img_idx)+'.png'\n",
    "                img = io.imread(path)\n",
    "                img = cv2.resize(img, (128, 128))\n",
    "                x_valid.append(img)\n",
    "\n",
    "        # Split corresponding output label data.\n",
    "        for folder_idx in range(start, end):\n",
    "            for img_idx in range(0, 150-validation_num):\n",
    "                y_train.append(np.array([folder_idx]))\n",
    "            for img_idx in range(150-validation_num, 150):\n",
    "                y_valid.append(np.array([folder_idx]))\n",
    "\n",
    "        # Convert list to numpy \n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train)\n",
    "        x_valid = np.array(x_valid)\n",
    "        y_valid = np.array(y_valid)\n",
    "\n",
    "        # TODO : Define train data and valid data directory path.\n",
    "        # TODO : Recommends not to change these directory paths. \n",
    "        train_save_dir = 'train_data'\n",
    "        valid_save_dir = 'valid_data'\n",
    "        if not os.path.exists(train_save_dir):\n",
    "            os.makedirs(train_save_dir)\n",
    "\n",
    "        if not os.path.exists(valid_save_dir):\n",
    "            os.makedirs(valid_save_dir)\n",
    "\n",
    "        # TODO : Save train/valid data\n",
    "        np.save(f'./train_data/x_data_{div_idx+1}', x_train)\n",
    "        np.save(f'./train_data/y_data_{div_idx+1}', y_train)\n",
    "        np.save(f'./valid_data/x_data_{div_idx+1}', x_valid)\n",
    "        np.save(f'./valid_data/y_data_{div_idx+1}', y_valid)\n",
    "\n",
    "        print(f\" ===================== Done in {div_idx} ===================== \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataloader (Do not change!)\n",
    "\n",
    "    You can define your own dataloader with API of torch.utils.data.Dataset.  \n",
    "    This can usually help you to reduce computational burden when dealing with high dimensional data, such as images.  \n",
    "\n",
    "    reference url : https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_data, y_data, device):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # .transpose(0, 2) : width x height x channel (0, 1, 2) ---> channel x width x height (2, 0, 1).\n",
    "        # .squeeze(0) : add extra dimension at axis 0.\n",
    "        x = torch.FloatTensor(self.x_data[idx]).transpose(0, 2)\n",
    "        y = torch.LongTensor(self.y_data[idx]).squeeze(0)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "def load_train_data(class_num):\n",
    "    # TODO : set 'class_path' with your train_data path.\n",
    "    class_path  = f'./train_data/'\n",
    "    x_data_path = class_path + 'x_data_' + str(class_num+1) + '.npy'\n",
    "    y_data_path = class_path + 'y_data_' + str(class_num+1) + '.npy'\n",
    "    x_data      = np.load(x_data_path, allow_pickle=True)\n",
    "    y_data      = np.load(y_data_path, allow_pickle=True)\n",
    "    return x_data, y_data\n",
    "\n",
    "def load_valid_data(class_num):\n",
    "    # TODO : set 'class_path' with your valid_data path.\n",
    "    class_path  = f'./valid_data/'\n",
    "    x_data_path = class_path + 'x_data_' + str(class_num+1) + '.npy'\n",
    "    y_data_path = class_path + 'y_data_' + str(class_num+1) + '.npy'\n",
    "    x_data      = np.load(x_data_path, allow_pickle=True)\n",
    "    y_data      = np.load(y_data_path, allow_pickle=True)\n",
    "\n",
    "    # return processed data. \n",
    "    return x_data, y_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define tranining function (You can modify this part!)\n",
    "\n",
    "    Set your model with train mode as 'model.train()'.   \n",
    "\n",
    "    useful reference : https://wikidocs.net/195118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, optimizer, num_epochs, train_data_loader, criterion):\n",
    "    \"\"\"\n",
    "    model             : your customized model \n",
    "    x_train           : input data for tranining\n",
    "    optimizer         : optimizer\n",
    "    num_epoches       : number of iteration\n",
    "    train_data_loader : dataloder of training dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()                     # Set train mode. \n",
    "    model.before_task()\n",
    "    for epoch in range(num_epochs):\n",
    "        acc      = 0 # Accuracy\n",
    "        avg_cost = 0 # Average Cost \n",
    "        for x, y in train_data_loader:\n",
    "            out = model(x.to(device))            # Inference\n",
    "            _, preds = torch.max(out, 1)         # preds : Predicted class\n",
    "            cost = model.get_cost(out, y.to(device))\n",
    "\n",
    "            # Optimize processs.\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_cost += cost # Average cost \n",
    "            acc      += torch.sum(preds.detach().cpu() == (y.data).detach().cpu()) # Accuracy\n",
    "        print(f\" # - EPOCHS {epoch + 1} / {num_epochs} | AvgCost {avg_cost} | Accuracy : {acc/len(x_train)} - #\")\n",
    "    \n",
    "    model.after_task(train_data_loader)\n",
    "\n",
    "\n",
    "    # Return trainded model and accuracy. \n",
    "    return model, acc/len(x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define validataion function (Do not change!)\n",
    "\n",
    "    And eval mode as 'model.eval()' or 'model.train(False)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, x_valid, valid_data_loader, criterion):\n",
    "    \"\"\"\n",
    "    model             : your customized model \n",
    "    x_vallid          : input data for validation\n",
    "    valid_data_loader : dataloder of valid dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() # Set eval mode\n",
    "    \n",
    "    acc = 0\n",
    "    \n",
    "    for x, y in valid_data_loader:\n",
    "        out = model(x.data.to(device))\n",
    "        _, preds = torch.max(out, 1)\n",
    "        cost  = criterion(out, y.to(device))\n",
    "        acc += torch.sum(preds.detach().cpu() == (y.data).detach().cpu())\n",
    "    print(f\" # - ValidCost {cost} | Accuracy : {acc / len(x_valid)} - #\")\n",
    "\n",
    "    # Return Accuracy \n",
    "    return acc/len(x_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your model and hyperparameter (You can modify this part!)\n",
    "\n",
    "    Here is the pivotal part of your competition.\n",
    "    We gives a simple CNN model, for example. \n",
    "    Go make your own model!         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channel, num_class):\n",
    "        super().__init__()\n",
    "        from torchvision.models import resnet18\n",
    "        self.backbone = resnet18()\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_class)\n",
    "\n",
    "        self.fisher = None\n",
    "        self.lamb = 80\n",
    "        self.fishermax = 0.0001\n",
    "        self._known_classes = 0\n",
    "        self._total_classes = 0\n",
    "        self._cur_task = -1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x/255.0\n",
    "        out = self.backbone(x)\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def before_task(self):\n",
    "        self._cur_task += 1\n",
    "        self._total_classes = self._known_classes + 100 \n",
    "\n",
    "    def after_task(self, train_loader):\n",
    "        if self.fisher is None:\n",
    "            self.fisher = self.getFisherDiagonal(train_loader)\n",
    "        else:\n",
    "            alpha = self._known_classes / self._total_classes\n",
    "            new_finsher = self.getFisherDiagonal(train_loader)\n",
    "            for n, p in new_finsher.items():\n",
    "                new_finsher[n][: len(self.fisher[n])] = (\n",
    "                    alpha * self.fisher[n]\n",
    "                    + (1 - alpha) * new_finsher[n][: len(self.fisher[n])]\n",
    "                )\n",
    "            self.fisher = new_finsher\n",
    "        self.mean = {\n",
    "            n: p.clone().detach()\n",
    "            for n, p in self.named_parameters()\n",
    "            if p.requires_grad\n",
    "        }\n",
    "        self._known_classes = self._total_classes\n",
    "\n",
    "    def getFisherDiagonal(self, train_loader):\n",
    "        fisher = {\n",
    "            n: torch.zeros(p.shape).to(device)\n",
    "            for n, p in self.named_parameters()\n",
    "            if p.requires_grad\n",
    "        }\n",
    "        self.train()\n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits = self.forward(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for n, p in self.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    fisher[n] += p.grad.pow(2).clone()\n",
    "        for n, p in fisher.items():\n",
    "            fisher[n] = p / len(train_loader)\n",
    "            fisher[n] = torch.min(fisher[n], torch.tensor(self.fishermax))\n",
    "        return fisher\n",
    "    \n",
    "    def get_cost(self, logits, targets):\n",
    "        if self._cur_task == 0:\n",
    "            return torch.nn.functional.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss_clf = torch.nn.functional.cross_entropy(\n",
    "                logits[:, self._known_classes :], targets - self._known_classes\n",
    "            )\n",
    "            loss_ewc = self.compute_ewc()\n",
    "            return loss_clf + self.lamb * loss_ewc\n",
    "\n",
    "    def compute_ewc(self):\n",
    "        loss = 0\n",
    "        for n, p in self.named_parameters():\n",
    "            if n in self.fisher.keys():\n",
    "                loss += (\n",
    "                    torch.sum(\n",
    "                        (self.fisher[n])\n",
    "                        * (p[: len(self.mean[n])] - self.mean[n]).pow(2)\n",
    "                    )\n",
    "                    / 2\n",
    "                )\n",
    "        return loss\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SimpleCNN(in_channel=3, num_class=1000).to(device)\n",
    "        \n",
    "# TODO : Set your hyperparameters\n",
    "batch_size        = 500\n",
    "learning_rate     = 0.1\n",
    "num_epochs        = 25\n",
    "optimizer         = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "random_seed       = 555\n",
    "validation_num    = 20 # for 150 images for class, the number for validation data\n",
    "criterion = nn.CrossEntropyLoss() # Define criterion. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Learning. (Do not change!)\n",
    "\n",
    "### WARNING:\n",
    "    The training and validation datasets each SHOULD BE prepared properly beforehand.  \n",
    "    If not, the submitted code from you will be immediately rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"  \n",
    "--div_data  : split your data or not.   \n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser()  \n",
    "parser.add_argument('--div_data',   default = False)  # Change this with 'False' after dividing your datsaet into 10 groups.\n",
    "args = parser.parse_args(args=[])  \n",
    "\n",
    "# TODO : Saving tranined model in this location. Don't change this path. \n",
    "save_dir = './result'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# TODO : Seed\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# TODO : Split dataset according to argument '--div_data'\n",
    "if args.div_data == True:\n",
    "    train_split(validation_num)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "1. training      : train each 100 classes sequentailly with respect to 1000 output class. \n",
    "    trainining class === 1-100 -> 101-200 -> 201-300 -> 301-400 -> ... -> 901-1000\n",
    "    \n",
    "2. validation    : validate each trained model.\n",
    "    validation class === 1-100 -> 1-200 -> 1-300 -> ... -> 1-1000\n",
    "    \n",
    "3. model save    : saves each trained model.                \n",
    "\"\"\"\n",
    "\n",
    "for div_idx in range(10):\n",
    "\n",
    "    # TODO : Load your train and validation data\n",
    "    x_train, y_train = load_train_data(div_idx)\n",
    "    x_valid, y_valid = load_valid_data(div_idx)\n",
    "\n",
    "    \"\"\"\n",
    "        in case of tranining 1  -100 classes, validate on 1-100 classes\n",
    "        in case of tranining 101-200 classes, validate on 1-200 classes\n",
    "        in case of tranining 201-300 classes, validate on 1-300 classes\n",
    "        and so on...            \n",
    "    \"\"\"\n",
    "    \n",
    "    if div_idx == 0:\n",
    "        x_val_tmp = x_valid\n",
    "        y_val_tmp = y_valid\n",
    "    else:\n",
    "        x_val_tmp = np.concatenate((x_val_tmp, x_valid), axis = 0)\n",
    "        y_val_tmp = np.concatenate((y_val_tmp, y_valid), axis = 0)\n",
    "        x_valid   = x_val_tmp\n",
    "        y_valid   = y_val_tmp\n",
    "\n",
    "    # TODO : let the label starts from 0 to match the output index of model prediction. (Currently the label starts from 1.)\n",
    "    y_train = y_train - 1\n",
    "    y_valid = y_valid - 1\n",
    "\n",
    "    # TODO : Define dataset and dataloader\n",
    "    train_dataset     = CustomDataset(x_train, y_train, device)\n",
    "    valid_dataset     = CustomDataset(x_valid, y_valid, device)\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_data_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # TODO : train and validate\n",
    "    trained_model, acc_train = train_model(model, x_train, optimizer, num_epochs, train_data_loader, criterion)\n",
    "    acc_valid                = validation(trained_model, x_valid, valid_data_loader, criterion)\n",
    "\n",
    "    if div_idx == 9:\n",
    "        MODEL_SAVE_FOLDER_PATH = './model_save/'\n",
    "        if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "            os.mkdir(MODEL_SAVE_FOLDER_PATH)        \n",
    "        model_path = MODEL_SAVE_FOLDER_PATH + 'continual_model.pt'\n",
    "        # TODO : save trained model in 'save_model_path'\n",
    "        torch.save(trained_model.state_dict(), model_path)\n",
    "\n",
    "    print(f'{str(div_idx)} Iteration Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
